{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">CIMPA School Research School \"Control, Optimization, and Model Reduction in Machine Learning\"</span>\n",
    "\n",
    "### <span style=\"color:rgb(139,69,19)\">Optimization for Machine Learning - C. W. Royer</span>\n",
    "\n",
    "\n",
    "# <span style=\"color:rgb(139,69,19)\">Lab 01 - Around gradient descent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(139,69,19)\">Preliminary remarks</span>\n",
    "\n",
    "- This notebook and the subsequent ones used in this course mix Python code and text/LaTeX blocks. They can be run offline on any computer where Python and Jupyter are installed, or online using Google Colab (requires a Google account).\n",
    "\n",
    "- All code blocks from this notebook are meant to be run in the order that they are given. In particular, the first block below must be run first in order to import the necessary toolboxes.\n",
    "\n",
    "- All the notebooks from this course rely on Python and the NumPy library. A basic yet very useful tutorial on NumPy is freely available\n",
    "[here](https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble: useful toolboxes, librairies, functions, etc.\n",
    "\n",
    "# Display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt # Square root\n",
    "from math import ceil # Ceil integer operator\n",
    "from math import log # Logarithm function\n",
    "\n",
    "# NumPy - Matrix and vector structures\n",
    "import numpy as np # NumPy library\n",
    "from numpy.random import multivariate_normal, randn, uniform # Probability distributions\n",
    "\n",
    "# SciPy - Efficient mathematical calculation\n",
    "from scipy.linalg import toeplitz # Toeplitz matrices\n",
    "from scipy.linalg import norm # Euclidean norm\n",
    "from scipy.linalg import svdvals # Singular value decomposition\n",
    "from scipy.linalg import qr # QR decomposition from linear algebra\n",
    "from scipy.optimize import check_grad # Numerical check of derivatives\n",
    "from scipy.optimize import fmin_l_bfgs_b # An efficient minimization routine in moderate dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">Introduction</span>\n",
    "\n",
    "In this session, we illustrate the behavior of gradient descent techniques on linear regression, arguably the most classical data analysis task. The nice structure of such problems (smoothness, convexity) allows to showcase the behavior of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">1 - Data and problem class</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset $\\{(\\mathbf{x}_i,y_i)\\}_{i=1,\\dots,n}$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$, available in the form of:\n",
    "\n",
    "- a feature matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$;\n",
    "- and a vector of labels $\\mathbf{y} \\in \\mathbb{R}^n$. \n",
    "\n",
    "Given this dataset, we will seek a linear model parameterized by a vector $\\mathbf{w}$ that explains the data according to a certain loss function $\\ell$. This results in the following formulation:\n",
    "$$\n",
    "    \\min_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{w}), \\qquad f_i(\\mathbf{w}) = \\ell(\\mathbf{x}_i^T\\mathbf{w},y_i) + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2_2,\n",
    "$$\n",
    "where $\\lambda \\ge 0$ is a regularization parameter associated with an $\\ell_2$ regularization term. In the first part of the notebook, we will use $\\lambda=0$, but provide the generic framework for subsequent sections.\n",
    "\n",
    "The dataset will be produced according to the procedure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation.\n",
    "# This code is inspired by a generator proposed by A. Gramfort from INRIA.\n",
    "\n",
    "def simu_linmodel(wtrue, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation values obtained by a linear model with additive noise\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    wtrue : np.ndarray, shape=(d,)\n",
    "        The true coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the feature matrix\n",
    "    \"\"\"    \n",
    "    d = wtrue.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    X = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    y = X.dot(wtrue) + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is thus produced from a linear model corrupted with noise: $\\mathbf{y} = \\mathbf{X} \\mathbf{w}^* + \\mathbf{\\epsilon}$, where $\\mathbf{\\epsilon}$ follows a Gaussian distribution. Our goal will thus be to learn a linear model from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">1.1 Linear regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *linear regression*, we seek a linear model that explains the data based on minimizing a least-squares objective:\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w}) \n",
    "    := \\frac{1}{2 n} \\|\\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2_2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2_2\n",
    "$$ \n",
    "for some $\\lambda \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Observations</span> \n",
    "\n",
    "- The function $f$ is quadratic.\n",
    "- The function $f$ is $L$-smooth with $L = \\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|_2}{n}+\\lambda$. \n",
    "- The function $f$ is convex, and $(\\sigma_{\\min}(\\mathbf{X})+\\lambda)$-strongly convex when $\\sigma_{\\min}(\\mathbf{X})+\\lambda>0$, where $\\sigma_{\\min}(\\mathbf{X})$ denotes the minimum singular value of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Mathematical details</span> \n",
    "\n",
    "One can rewrite the objective function as\n",
    "$$\n",
    "    f(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^T \\left(\\frac{\\mathbf{X}^T \\mathbf{X}}{n} + \\lambda \\mathbf{I}_d\\right)\\mathbf{w} - \\frac{1}{n}\\mathbf{y}^T \\mathbf{X} \\mathbf{w} + \\frac{1}{2} \\mathbf{y}^T \\mathbf{y}.\n",
    "$$\n",
    "The gradient of the objective function above is given by:\n",
    "$$\n",
    "    \\nabla f(\\mathbf{w}) = \\left( \\frac{\\mathbf{X}^T \\mathbf{X}}{n} + \\lambda \\mathbf{I}_d \\right)\\mathbf{w} - \\frac{1}{n}\\mathbf{X}^T \\mathbf{y}\n",
    "    = \\frac{1}{n}\\mathbf{X}^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) + \\lambda \\mathbf{w}.\n",
    "$$\n",
    "For any $\\mathbf{w},\\mathbf{v} \\in \\mathbb{R}^d$, the formula for the gradient gives:\n",
    "$$\n",
    "    \\|\\nabla f(\\mathbf{w}) -\\nabla f(\\mathbf{v})\\| \n",
    "    = \\left\\|\\frac{\\mathbf{X}^T \\mathbf{X}}{n} \\mathbf{w} + \\lambda \\mathbf{w} - \\frac{\\mathbf{X}^T \\mathbf{X}}{n}\\mathbf{v} - \\lambda \\mathbf{v} \\right\\|_2 \n",
    "    \\le \\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|_2}{n}\\|\\mathbf{w}-\\mathbf{v}\\|_2 + \\lambda \\|\\mathbf{w}-\\mathbf{v}\\|_2.\n",
    "$$\n",
    "The gradient $\\nabla f$ is thus $\\left(\\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|_2}{n}+\\lambda\\right)$-Lipschitz continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">1.2 Logistic regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *logistic regression*, we still consider a linear model, but for classification purposes (here $y_i \\in \\{-1,1\\}$). We use a loss function better suited for classification:\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w}) \n",
    "    := \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{w}), \\qquad \n",
    "    f_i(\\mathbf{w})=\\log(1+\\exp(-y_i \\mathbf{x}_i^T \\mathbf{w}))+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2_2,\n",
    "$$\n",
    "where $\\lambda \\ge 0$ is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Observations</span> \n",
    "\n",
    "- For any $\\mathbf{w} \\in \\mathbb{R}^d$, one has\n",
    "$$\n",
    "\\nabla f(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n  -\\frac{y_i}{1 + \\exp(y_i \\mathbf{x}_i^T \\mathbf{w})} \\mathbf{x}_i + \\lambda \\mathbf{w}.\n",
    "$$\n",
    "- The function $f$ is convex and even $\\lambda$-strongly convex when $\\lambda>0$.\n",
    "- The function $f$ is $L$-smooth with $L =\\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|_2}{4n}+\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Details</span> \n",
    "\n",
    "1. The derivative of $t \\mapsto \\log(1+\\exp(-t))$ is $t \\mapsto \\frac{-\\exp(-t)}{1+\\exp(-t)} = -\\frac{1}{1+\\exp(t)}$. Combining this with the linear function $\\mathbf{w} \\mapsto y_i \\mathbf{x}_i^T \\mathbf{w}$, we get for every $i$ that\n",
    "$$\n",
    "\\nabla f_i(\\mathbf{w}) = - \\frac{y_i}{1 + \\exp(y_i \\mathbf{x}_i^T \\mathbf{w})} \\mathbf{x}_i + \\lambda \\mathbf{w}.\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\nabla f(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n  -\\frac{y_i}{1 + \\exp(y_i \\mathbf{x}_i^T \\mathbf{w})} \\mathbf{x}_i + \\lambda \\mathbf{w}\n",
    "$$\n",
    "2. One way to obtain a Lipschitz constant for the gradient is to bound the second-order derivative, which is given by:\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n  \n",
    "\\frac{\\exp(-y_i \\mathbf{x}_i^T \\mathbf{w})}{(1 + \\exp(-y_i \\mathbf{x}_i^T \\mathbf{w}))^2} \\mathbf{x}_i \\mathbf{x}_i^T + \\lambda I,\n",
    "$$\n",
    "where $I$ is the identity matrix. The result follows by noticing that $t \\mapsto \\tfrac{e^{t}}{(1+\\exp(t))^2}$  is always less than or equal to $\\tfrac{1}{4}$ (its value at the origin), we obtain that\n",
    "$$\n",
    "\\| \\nabla^2 f(\\mathbf{w}) \\| \\le L:=\\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|_2}{4n}+\\lambda,\n",
    "$$\n",
    "hence $\\nabla f$ is $L$-Lipschitz continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">1.3 Python class for regression problems</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python class below encodes the objective and the gradient of both regression problems, using the formulas from the previous sections. Note that formulas for the Lipschitz constants as well as a strong convexity parameter are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python class for regression problems\n",
    "class RegPb(object):\n",
    "    '''\n",
    "        A class for regression problems with linear models.\n",
    "        \n",
    "        Attributes:\n",
    "            X: Data matrix (features)\n",
    "            y: Data vector (labels)\n",
    "            n,d: Dimensions of X\n",
    "            loss: Loss function to be considered in the regression\n",
    "                'l2': Least-squares loss\n",
    "                'logit': Logistic loss\n",
    "            lbda: Regularization parameter\n",
    "    '''\n",
    "   \n",
    "    # Instantiate the class\n",
    "    def __init__(self, X, y,lbda=0,loss='l2'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "        self.loss = loss\n",
    "        self.lbda = lbda\n",
    "        \n",
    "    \n",
    "    # Objective value\n",
    "    def fun(self, w):\n",
    "        if self.loss=='l2':\n",
    "            return norm(self.X.dot(w) - self.y) ** 2 / (2. * self.n) + self.lbda * norm(w) ** 2 / 2.\n",
    "        elif self.loss=='logit':\n",
    "            yXw = self.y * self.X.dot(w)\n",
    "            return np.mean(np.log(1. + np.exp(-yXw))) + self.lbda * norm(w) ** 2 / 2.\n",
    "\n",
    "    \n",
    "    # Full gradient computation\n",
    "    def grad(self, w):\n",
    "        if self.loss=='l2':\n",
    "            return self.X.T.dot(self.X.dot(w) - self.y) / self.n + self.lbda * w\n",
    "        elif self.loss=='logit':\n",
    "            yXw = self.y * self.X.dot(w)\n",
    "            aux = 1. / (1. + np.exp(yXw))\n",
    "            return - (self.X.T).dot(self.y * aux) / self.n + self.lbda * w\n",
    "    \n",
    "\n",
    "    # Lipschitz constant for the gradient\n",
    "    def lipgrad(self):\n",
    "        if self.loss=='l2':\n",
    "            L = norm(self.X, ord=2) ** 2 / self.n + self.lbda\n",
    "        elif self.loss=='logit':\n",
    "            L = norm(self.X, ord=2) ** 2 / (4. * self.n) + self.lbda\n",
    "        return L\n",
    "    \n",
    "    # ''Strong'' convexity constant (could be zero if self.lbda=0)\n",
    "    def cvxval(self):\n",
    "        if self.loss=='l2':\n",
    "            s = svdvals(self.X)\n",
    "            mu = min(s)**2 / self.n # More efficient than computing ||A^T A||\n",
    "            return mu + self.lbda\n",
    "        elif self.loss=='logit':\n",
    "            return self.lbda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first script below generates two problem instances (one for each class), the second checks the implementation of the derivatives, and the third one computes an approximate solution for each problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the problem instances - we use moderate sizes but those will serve our purpose\n",
    "\n",
    "d = 50\n",
    "n = 1000\n",
    "idx = np.arange(d)\n",
    "lbda = 1. / n ** (0.5)\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Ground truth coefficients of the model, chosen so that the magnitude of the entries decays\n",
    "w_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "Xlin, ylin = simu_linmodel(w_model_truth, n, std=1., corr=0.1)\n",
    "Xlog, ylog = simu_linmodel(w_model_truth, n, std=1., corr=0.7)\n",
    "ylog = np.sign(ylog) # Taking the logarithm for binary classification\n",
    "\n",
    "pblinreg = RegPb(Xlin, ylin,lbda,loss='l2')\n",
    "pblogreg = RegPb(Xlog, ylog,lbda,loss='logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">1.4 Numerical estimates of $\\min$ and $\\mathrm{argmin}$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we work with relatively simple loss functions (and a moderate number of data points): we can thus efficiently compute a solution using a second-order method. This provides us with a target objective value as well as a target vector of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use L-BFGS-B to determine a solution for both problems\n",
    "\n",
    "w_init = np.zeros(d)\n",
    "# Compute the optimal solution for linear regression\n",
    "w_min_lin, f_min_lin, _ = fmin_l_bfgs_b(pblinreg.fun, w_init, pblinreg.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "print(\"Linear regression:\")\n",
    "print(\"\\t Numerical minimal value:\",f_min_lin)\n",
    "print(\"\\t Numerical minimum gradient norm:\",norm(pblinreg.grad(w_min_lin)))\n",
    "\n",
    "# Compute the optimal solution for logistic regression\n",
    "w_min_log, f_min_log, _ = fmin_l_bfgs_b(pblogreg.fun, w_init, pblogreg.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "print(\"Logistic regression:\")\n",
    "print(\"\\t Numerical minimal value:\",f_min_log)\n",
    "print(\"\\t Numerical minimum gradient norm:\",norm(pblogreg.grad(w_min_log)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These solutions will enable us to study the behavior of the distance to optimality in terms of function values \n",
    "$f(\\mathbf{w}_k)-f^*$ and iterates $\\|\\mathbf{w}_k -\\mathbf{w}^*\\|$. \n",
    "\n",
    "*Note: Since $\\|\\nabla f(\\mathbf{w}_k)\\|^2 \\ge 2 \\mu (f(\\mathbf{w}_k)-f^*)$ for a $\\mu$-strongly convex, continuously differentiable function with optimal value $f^*$, the gradient could also be used as an upper estimate of the distance to optimality in terms of function values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">2 - Gradient descent implementation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will investigate several techniques for selecting the stepsize in a predetermined or adaptive fashion in gradient descent. Those are by no means exhaustive, but give a nice overview of existing options (see upcoming lectures for more details about the theory behind those methods). We will consider the three following options:\n",
    "\n",
    "- *Constant stepsize:* $\\alpha_k = \\frac{\\bar{\\alpha}}{L}$, where $L$ is the Lipschitz constant for $\\nabla f$ (assumed to be known in that case).\n",
    "\n",
    "- *Decreasing stepsize:* $\\alpha_k = \\frac{\\bar{\\alpha}}{(k+1)^a}$, where $a>0$ is given as input\n",
    "\n",
    "- *Armijo-type line search:* $\\alpha_k=\\frac{\\bar{\\alpha}}{2^{j_k}}$, where $j_k$ is the smallest nonnegative integer such that\n",
    "$$\n",
    "f\\left(\\mathbf{w}_k-\\frac{\\bar{\\alpha}}{2^{j_k}}\\nabla f(\\mathbf{w}_k)\\right) < f(\\mathbf{w}_k) - 0.0001 \\frac{\\alpha}{2^{j_k}}\\|\\nabla f(\\mathbf{w}_k)\\|^2_2.\n",
    "$$\n",
    "In practice, the line search should stop if the decrease condition is satisfied or drops below a certain value, e.g. $\\frac{\\bar{\\alpha}}{2^{j_k}} < 10^{-10}$. Note that the parameters $2$,$0.0001$ and $10^{-10}$ are set for simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are implemented in the template algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "def gd_algo(w0,problem,wopt,stepchoice=0,stepbar=1, n_iter=1000, verbose=False): \n",
    "    \"\"\"\n",
    "        An implementation of gradient descent with several stepsize rules.\n",
    "        \n",
    "        Inputs:\n",
    "            w0: Initial point\n",
    "            problem: Problem structure\n",
    "                problem.fun(w) Objective value\n",
    "                problem.grad(w) Gradient\n",
    "                problem.lipgrad() Lipschitz constant for the gradient\n",
    "            wopt: Target point for the optimization (approximate optimum computed beforehand)\n",
    "            stepchoice: Rule for choosing the stepsize (see above)\n",
    "                0: Constant equal to 1/L where L is a Lipschitz constant for the gradient\n",
    "                a>0: Decreasing, set to 1/((k+1)**a)\n",
    "                -1: Armijo line search\n",
    "            stepbar: Initial step size (used when stepchoice = 1)\n",
    "            n_iter: Maximum iteration number\n",
    "            verbose: Boolean value indicating whether iteration-level information should be displayed.\n",
    "      \n",
    "        Outputs:\n",
    "            w_output: Last iterate of the method\n",
    "            objvals: History of function values (Numpy array of length n_iter)\n",
    "            distits: History of distances to the target point (Numpy array of length n_iter)\n",
    "            ngvals: History of gradient norms (Numpy array of length n_iter)\n",
    "            stepsize_variant: String describing the stepsize variant\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    ############\n",
    "    # Initialization\n",
    "\n",
    "    # History of function values\n",
    "    objvals = []\n",
    "    \n",
    "    # History of gradient norms\n",
    "    ngvals = []\n",
    "    \n",
    "    # History of distances to the target\n",
    "    distits = []\n",
    "    \n",
    "    # Lipschitz constant\n",
    "    L = problem.lipgrad()\n",
    "    \n",
    "    # Initial value of the incumbent, a.k.a. current iterate\n",
    "    w = w0.copy()\n",
    "\n",
    "    # Initialize the iteration count\n",
    "    k=0    \n",
    "    \n",
    "    # Initial function value\n",
    "    obj = problem.fun(w) \n",
    "    objvals.append(obj);\n",
    "    \n",
    "    # Initial gradient\n",
    "    g = problem.grad(w)\n",
    "    ng = norm(g)\n",
    "    ngvals.append(ng)\n",
    "    \n",
    "    # Distance between the current point and the optimal point\n",
    "    dist = norm(w-wopt)\n",
    "    distits.append(dist)\n",
    "    \n",
    "    # Label for stepsize choice\n",
    "    if stepchoice==0:\n",
    "        stepsize_variant=\"Cst \"+f\"{stepbar/L:.2f}\"\n",
    "    elif stepchoice>0:\n",
    "        stepsize_variant=\"Dec \"+f\"{stepbar:.2f}\"+\"/(k+1)^\"+f\"{stepchoice:.2f}\"\n",
    "    else:\n",
    "        stepsize_variant=\"Lis (Init \"+f\"{stepbar:.2f}\"+\")\"\n",
    "\n",
    "    # Plot initial values \n",
    "    if verbose:\n",
    "        print(\"Gradient descent:\")\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"dist\",\"stepsize\"]]))\n",
    "    \n",
    "    ####################\n",
    "    # Main loop\n",
    "    while (k < n_iter):\n",
    "        \n",
    "        #####################################################################################\n",
    "        \n",
    "        # 1 - Define the stepsize s based on k (iteration index), L (Lipschitz constant), step0 (initial value)\n",
    "        # and g (the function)\n",
    "        if stepchoice==0:\n",
    "            # Constant stepsize\n",
    "            s = (stepbar/L)\n",
    "        elif stepchoice>0:\n",
    "            # Decreasing stepsize\n",
    "            s = stepbar/((k+1)**stepchoice)\n",
    "        elif stepchoice==-1:\n",
    "            # Line search (inner while loop)\n",
    "            s = stepbar\n",
    "            while (problem.fun(w-s*g) >= obj - 0.0001*s*(ng**2)) and (s>1e-10):\n",
    "                s = 0.5*s\n",
    "        \n",
    "        # 2 - Perform the gradient descent iteration using the stepsize s and the gradient g\n",
    "        w[:] = w - s*g\n",
    "      \n",
    "        \n",
    "        ####################################################################################\n",
    "        \n",
    "        \n",
    "        # Plot relevant information\n",
    "        if verbose:        \n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % dist).rjust(8),(\"%.2e\" % s).rjust(8)]))\n",
    "        \n",
    "        # Compute values associated with the next iterate\n",
    "        obj = problem.fun(w)\n",
    "        objvals.append(obj)\n",
    "        g = problem.grad(w)\n",
    "        ng = norm(g)\n",
    "        ngvals.append(ng)\n",
    "        dist = norm(w-wopt)\n",
    "        distits.append(dist)\n",
    "        \n",
    "        # Increase iteration counter\n",
    "        k += 1\n",
    "    \n",
    "    # End main loop\n",
    "    ######################\n",
    "    \n",
    "    # Outputs\n",
    "    w_output = w.copy()\n",
    "    return w_output, np.array(objvals), np.array(distits), np.array(ngvals),stepsize_variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,11)\">Hands-on! Experiments on linear regression</span>\n",
    "\n",
    "**a) Run gradient descent when using the four following rules on linear regression:**\n",
    "\n",
    "- $\\alpha_k = \\frac{1}{L}$;\n",
    "\n",
    "- $\\alpha_k = \\frac{1}{k+1}$;\n",
    "\n",
    "- $\\alpha_k = \\frac{1}{\\sqrt{k+1}}$;\n",
    "\n",
    "- $\\alpha_k$ **chosen through Armijo line search with $\\bar{\\alpha}=1$.**\n",
    "\n",
    "**b)Compare the convergence curves in terms of function values $\\{f(x_k)\\}$ and $\\{f(x_k)-f^*_{lin}\\}$, where \n",
    "$f^*_{lin}$ is the value computed in Section 1.**\n",
    "\n",
    "**c) Plot the gradient norms and the distances to optimum as a function of the number of iterations. Are those plots consistent with the others? Do they bring additional insights about the methods?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a)\n",
    "\n",
    "# Run four variants of gradient descent using the same initial point\n",
    "w0 = np.zeros(d)\n",
    "_, obj_a, dist_a, ngrad_a, sv_a = gd_algo(w0,pblinreg,w_min_lin,stepchoice=0,stepbar=1, n_iter=50)\n",
    "_, obj_b, dist_b, ngrad_b, sv_b = gd_algo(w0,pblinreg,w_min_lin,stepchoice=1,stepbar=1, n_iter=50)\n",
    "_, obj_c, dist_c, ngrad_c, sv_c = gd_algo(w0,pblinreg,w_min_lin,stepchoice=0.5,stepbar=1, n_iter=50)\n",
    "_, obj_d, dist_d, ngrad_d, sv_d = gd_algo(w0,pblinreg,w_min_lin,stepchoice=-1,stepbar=1, n_iter=50)\n",
    "\n",
    "# Best/Final objective value\n",
    "print(\"Final objective value for GD -\"+sv_a+\" \\t\",obj_a[-1])\n",
    "print(\"Final objective value for GD -\"+sv_b+\" \\t\",obj_b[-1])\n",
    "print(\"Final objective value for GD -\"+sv_c+\" \\t\",obj_c[-1])\n",
    "print(\"Final objective value for GD -\"+sv_d+\" \\t\",obj_d[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b)\n",
    "\n",
    "# We begin by plotting the change in the function value (in log scale) as a function of the iteration number\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_a, label=\"GD \"+sv_a, lw=2)\n",
    "plt.semilogy(obj_b, label=\"GD \"+sv_b, lw=2)\n",
    "plt.semilogy(obj_c, label=\"GD \"+sv_c, lw=2)\n",
    "plt.semilogy(obj_d, label=\"GD \"+sv_d, lw=2)\n",
    "plt.title(\"Objective behavior\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Objective function (log)\", fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# We can also plot the change in function value relatively to the target value \n",
    "# Note: In log scale, the values f(w)-vmin close to 0 do not appear\n",
    "\n",
    "\n",
    "#### Plot the behavior of all four methods in terms of relative objective value\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_a-f_min_lin, label=\"GD \"+sv_a, lw=2)\n",
    "plt.semilogy(obj_b-f_min_lin, label=\"GD \"+sv_b, lw=2)\n",
    "plt.semilogy(obj_c-f_min_lin, label=\"GD \"+sv_c, lw=2)\n",
    "plt.semilogy(obj_d-f_min_lin, label=\"GD \"+sv_d, lw=2)\n",
    "plt.title(\"Objective-Optimum behavior\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Objective function-Target (log)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c)\n",
    "\n",
    "#### Plot the behavior of all four methods in terms of gradient norm\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(ngrad_a, label=\"GD \"+sv_a, lw=2)\n",
    "plt.semilogy(ngrad_b , label=\"GD \"+sv_b, lw=2)\n",
    "plt.semilogy(ngrad_c, label=\"GD \"+sv_c, lw=2)\n",
    "plt.semilogy(ngrad_d, label=\"GD \"+sv_d, lw=2)\n",
    "plt.title(\"Gradient norm behavior\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Gradient norm (log)\", fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "#### Plot the behavior of all four methods in terms of distance to numerical optimum\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(dist_a, label=\"GD \"+sv_a, lw=2)\n",
    "plt.semilogy(dist_b , label=\"GD \"+sv_b, lw=2)\n",
    "plt.semilogy(dist_c, label=\"GD \"+sv_c, lw=2)\n",
    "plt.semilogy(dist_d, label=\"GD \"+sv_d, lw=2)\n",
    "plt.title(\"Distance to numerical minimum\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance (log)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands-on! Experiments on logistic regression</span>\n",
    "\n",
    "**a) Run the code below to try out a few values for the stepsize.**\n",
    "\n",
    "**b) Uncomment the last lines and try to find the best possible rule for the stepsize on logistic regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Experiments on logistic regression\n",
    "###############################################\n",
    "\n",
    "# Plotting the behavior of all methods\n",
    "# Run four variants of gradient descent using the same initial point\n",
    "# For the last variant, try to improve over the previous four\n",
    "w0 = np.zeros(d)\n",
    "_, obj_a, dist_a, ngrad_a, sv_a = gd_algo(w0,pblogreg,w_min_log,stepchoice=?,stepbar=?, n_iter=50)\n",
    "_, obj_b, dist_b, ngrad_b, sv_b = gd_algo(w0,pblogreg,w_min_log,stepchoice=?,stepbar=?, n_iter=50)\n",
    "_, obj_c, dist_c, ngrad_c, sv_c = gd_algo(w0,pblogreg,w_min_log,stepchoice=?,stepbar=?, n_iter=50)\n",
    "_, obj_d, dist_d, ngrad_d, sv_d = gd_algo(w0,pblogreg,w_min_log,stepchoice=?,stepbar=?, n_iter=50)\n",
    "_, obj_e, dist_e, ngrad_e, sv_e = gd_algo(w0,pblogreg,w_min_log,stepchoice=-1,stepbar=10, n_iter=50)\n",
    "\n",
    "\n",
    "#### Plot the behavior of all four methods in terms of relative objective value\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_a-f_min_log, label=\"GD \"+sv_a, lw=2)\n",
    "plt.semilogy(obj_b-f_min_log, label=\"GD \"+sv_b, lw=2)\n",
    "plt.semilogy(obj_c-f_min_log, label=\"GD \"+sv_c, lw=2)\n",
    "plt.semilogy(obj_d-f_min_log, label=\"GD \"+sv_d, lw=2)\n",
    "plt.semilogy(obj_e-f_min_log, label=\"GD \"+sv_e, lw=2)\n",
    "plt.title(\"Objective-Optimum behavior\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Objective function-Target (log)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">3 - Regularization</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we focus on linear regression problems, and consider the use of several regularization terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">3.1 $\\ell_1$ regularization</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LASSO estimator is defined as the solution of\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d}\\ f(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_1,\n",
    "    \\quad \\mathrm{with} \\quad\n",
    "    f(\\mathbf{w}):= \\frac{1}{2 n} \\|\\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2_2\n",
    "$$ \n",
    "Here $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and $\\mathbf{y} \\in \\mathbb{R}^n$ define the data-fitting part of the data, and $\\lambda \\ge 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation from a sparse ground truth \n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Ground truth with only 5% nonzero entries (built from the previously used ground truth)\n",
    "d = 200\n",
    "n = 200\n",
    "s = round(0.95*min(d,n))\n",
    "idx = np.arange(d)\n",
    "wtrue = (-1)**idx * np.exp(-idx / 10.)\n",
    "ip = np.random.permutation(d)\n",
    "wtrue[ip[0:s]]=0\n",
    "\n",
    "\n",
    "# Columns of X follow a normal distribution N(0,1/n)\n",
    "\n",
    "X = multivariate_normal(np.zeros(d), (1/n)*np.identity(d), size=n)\n",
    "Xw = X.dot(wtrue)\n",
    "std = (1/n)*(norm(Xw)**2)\n",
    "noise = std * randn(n)\n",
    "y = Xw + noise\n",
    "\n",
    "# Instantiate the (smooth) part of the problem\n",
    "pbsparse = RegPb(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">3.2 Proximal gradient/ISTA</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the basics of the *Iterative Soft-Thresholding Algorithm* (ISTA) below.\n",
    "\n",
    "Given an iterate $\\mathbf{x}_k$ and a positive stepsize $\\alpha_k$, the gradient step with respect to the smooth \n",
    "part of the objective is given by:\n",
    "$$\n",
    "    \\mathbf{g}_k = \\mathbf{w}_k - \\alpha_k \\nabla f(\\mathbf{w}_k),\n",
    "$$\n",
    "where we recall that $f$ denotes the smooth part of \n",
    "$f_{\\ell_1}(\\mathbf{w}) = f(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_1$.\n",
    "\n",
    "An iteration of proximal gradient applies the proximal operator \n",
    "$\\mathbf{w}_{k+1}=\\mbox{prox}_{\\alpha_k\\lambda\\|\\cdot\\|_1}(\\mathbf{g}_k)$, which is equivalent to\n",
    "$$\n",
    "    \\mathbf{w}_{k+1} \\in \\mathrm{argmin}_{\\mathbf{w} \\in \\mathbb{R}^d} \\left\\{ f(\\mathbf{w}_k) + \\nabla f(\\mathbf{w}_k)^\\top (\\mathbf{w}-\\mathbf{w}_k) + \\frac{1}{2\\alpha_k}\\|\\mathbf{w}-\\mathbf{w}_k\\|^2 + \\lambda \\|\\mathbf{w}\\|_1 \\right\\}. \n",
    "$$ \n",
    " \n",
    "The proximal subproblem solution has a close form, that can be stated componentwise as:\n",
    "$$\n",
    "    \\forall i=1,\\dots,d, \\quad [\\mathbf{w}_{k+1}]_i \\; = \\; \n",
    "    \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            [\\mathbf{g}_k]_i + \\lambda \\alpha_k &\\mathrm{if\\ } [\\mathbf{g}_k]_i < -\\lambda \\alpha_k \\\\\n",
    "            [\\mathbf{g}_k]_i - \\lambda \\alpha_k &\\mathrm{if\\ } [\\mathbf{g}_k]_i > \\lambda \\alpha_k \\\\\n",
    "            0 &\\mathrm{otherwise.}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of ISTA\n",
    "def ista(w0,problem,lbda,stepchoice=0,step0=1, n_iter=1000,verbose=False): \n",
    "    \"\"\"\n",
    "        A code for ISTA with various step choices.\n",
    "        \n",
    "        Inputs:\n",
    "            w0: Initial vector\n",
    "            problem: Problem structure\n",
    "                problem.fun(x) evaluates the objective function (which is assumed to be a finite sum of functions) at a given vector w\n",
    "                problem.grad(x) evaluates the gradient of the smooth part of the objective function at a vector w\n",
    "                problem.lipgrad() returns the Lipschitz constant for the gradient\n",
    "                problem.cvxval() returns the strong convexity constant\n",
    "            lbda: Regularization parameter\n",
    "            stepchoice: Strategy for computing the stepsize \n",
    "                0: Constant step size equal to step0/L\n",
    "                1: Step size decreasing in step0/(k+1)\n",
    "                2: Step size decreasing in step0/sqrt(k+1)\n",
    "            step0: Initial steplength (only used when stepchoice is not 0)\n",
    "            n_iter: Number of iterations\n",
    "            \n",
    "        Outputs:\n",
    "            w_output: Final iterate of the method\n",
    "            objvals: History of function values (output as a Numpy array of length n_iter at most)\n",
    "    \"\"\"   \n",
    "    \n",
    "    \n",
    "    ############\n",
    "    # Initial step: Compute and plot some initial quantities\n",
    "\n",
    "    # objective history\n",
    "    objvals = []\n",
    "    \n",
    "    # Lipschitz constant\n",
    "    L = problem.lipgrad()\n",
    "    \n",
    "    # Initial value of current iterate   \n",
    "    w = w0.copy()\n",
    "\n",
    "    # Initialize iteration counter\n",
    "    k=0    \n",
    "    \n",
    "    # Current objective\n",
    "    obj = problem.fun(w) \n",
    "    objvals.append(obj);\n",
    "\n",
    "    step=step0 # Initialize constant stepsize\n",
    "    threshold=0 # Initialize threshold for proximal step\n",
    "    \n",
    "    # Display initial quantities\n",
    "    if verbose:\n",
    "        print(\"ISTA:\")\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8)]))\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # Main loop\n",
    "    while (k < n_iter):\n",
    "        # Compute the gradient of the smooth part\n",
    "        g = problem.grad(w)\n",
    "        # Select the stepsize\n",
    "        if stepchoice==0:\n",
    "            step = 1/L\n",
    "        elif (stepchoice==1):\n",
    "            step = step0/(k+1)\n",
    "        else:\n",
    "            step = step0/(sqrt(k+1))\n",
    "        \n",
    "        # Compute the proximal gradient step\n",
    "        for i in range(problem.d):\n",
    "            vali = w[i]-step*g[i]\n",
    "            threshold = step*lbda\n",
    "            if vali < -threshold:\n",
    "                w[i] = vali+threshold\n",
    "            elif vali > threshold:\n",
    "                w[i] = vali-threshold\n",
    "            else:\n",
    "                w[i] = 0\n",
    "            \n",
    "        # Update objective value and iteration index\n",
    "        obj = problem.fun(w)\n",
    "        objvals.append(obj);\n",
    "        k += 1\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8)]))       \n",
    "        \n",
    "    # Output \n",
    "    w_output = w.copy()\n",
    "          \n",
    "    return w_output, np.array(objvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands-on! ISTA</span>\n",
    "\n",
    "**Try out the proximal gradient method with several values of $\\lambda$ in $[0,1]$. Can you \n",
    "find a good value for this parameter?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing l1 regularization\n",
    "#\n",
    "# ADD VALUES FOR lbda HERE!\n",
    "lvals = []\n",
    "nlbda = len(lvals)\n",
    "w0 = np.ones(d)\n",
    "Wsol = np.zeros((d,nlbda))\n",
    "dist_truth = np.zeros(nlbda)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i in range(nlbda):\n",
    "    lbda =lvals[i]\n",
    "    Wsol[:,i], obj_is = ista(w0,pbsparse,lbda,stepchoice=0,step0=1, n_iter=100)\n",
    "    dist_truth[i] = norm(Wsol[:,i]-wtrue,1)\n",
    "    print(f'Nonzero coefficients with lbda={lbda:.2e}'+f': {np.count_nonzero(Wsol[:,i]):d}')\n",
    "    print(f'Difference with truth, lbda={lbda:.2e}'+f': {dist_truth[i]:.2e}')\n",
    "    plt.semilogy(obj_is, label=f'lbda={lbda:2e}', lw=2)\n",
    "plt.title(\"Performance of ISTA\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend(loc=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i in ip[s:]:\n",
    "    plt.semilogx(lvals,Wsol[i,:],label=\"w_\"+str(i),c=np.random.rand(3,), lw=2)\n",
    "plt.title(\"ISTA solutions\", fontsize=16)\n",
    "plt.xlabel(\"Regularization\", fontsize=14)\n",
    "plt.ylabel(\"Magnitude\", fontsize=14)\n",
    "plt.legend(ncol=4,loc=1)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.semilogx(lvals,dist_truth/norm(wtrue,1),lw=2)\n",
    "plt.title(\"Error to ground truth\",fontsize=16)\n",
    "plt.xlabel(\"Regularization\", fontsize=14)\n",
    "plt.ylabel(\"Magnitude\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">4 - Robust linear regression and subgradient method</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we look at a linear regression problem based on a nonsmooth loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset $(\\mathbf{X},\\mathbf{y})$ for regression, we again aim at computing a linear model parameterized by a vector $\\mathbf{w} \\in \\mathbb{R}^d$. In the previous sections, we considered the least-squares loss\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} \n",
    "    \\frac{1}{2n}\\left\\| \\mathbf{X}\\mathbf{w}-\\mathbf{y} \\right\\|_2^2 \n",
    "    =\\frac{1}{2n}\\sum_{i=1}^n (\\mathbf{x}_i^T \\mathbf{w}-y_i)^2.\n",
    "$$\n",
    "\n",
    "When the dataset contains outliers, linear regression can produce poor results. An alternate optimization formulation, less sensitive to outliers, consists in replacing the $\\ell_2$ norm by the $\\ell_1$ norm in the objective, yielding\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} \n",
    "    \\frac{1}{n}\\left\\| \\mathbf{X}\\mathbf{w}-\\mathbf{y} \\right\\|_1 \n",
    "    = \\frac{1}{n}\\sum_{i=1}^n |\\mathbf{x}_i^T \\mathbf{w}-y_i|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">4.1 Dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a dataset with outliers, which can be observed directly in dimension $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "# This generating technique is adapted from J. Duchi (Stanford)\n",
    "def data_outliers(d, n, rng):\n",
    "    \n",
    "    \"\"\"\n",
    "    Linear model with a proportion of outliers\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "        d : Parameters\n",
    "    \n",
    "        n : Samples\n",
    "        \n",
    "        rng : Random number generator\n",
    "        \n",
    "    Outputs\n",
    "    ----------\n",
    "    \n",
    "        X,y: Feature matrix and label vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    X = rng.multivariate_normal(np.zeros(d), np.eye(d), size=n)\n",
    "    u = rng.multivariate_normal(np.zeros(d), np.eye(d))\n",
    "    noise = rng.normal(0.0,1.0,size=n)\n",
    "    y = X.dot(u) + noise*np.power(np.abs(noise),3)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of 1D data\n",
    "rng= np.random.default_rng(4)\n",
    "\n",
    "x0,y0=data_outliers(1,200,rng)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(x0,y0, label=\"Samples\", lw=2)\n",
    "plt.title(\"Dataset with outliers\", fontsize=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will use a higher-dimensional (yet still small-dimensional) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to be used in the next section\n",
    "\n",
    "d=50\n",
    "n=100\n",
    "\n",
    "rng= np.random.default_rng(4)\n",
    "\n",
    "X,y=data_outliers(d,n,rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any $\\mathbf{w} \\in \\mathbb{R}^d$, a subgradient of $f$ is given by\n",
    "$$\n",
    "    g(\\mathbf{w}) = \\frac{1}{n}\\mathbf{X}^T \\mbox{sign}(\\mathbf{X}\\mathbf{w}-\\mathbf{y}), \n",
    "    \\qquad \n",
    "    \\mbox{sign}(\\mathbf{X}\\mathbf{w}-\\mathbf{y})\n",
    "    =\n",
    "    \\begin{bmatrix} \n",
    "    \\mbox{sign}(\\mathbf{x}_1^T \\mathbf{w}-y_1) \\\\ \n",
    "    \\vdots \\\\\n",
    "    \\mbox{sign}(\\mathbf{x}_n^T \\mathbf{w}-y_n)\n",
    "    \\end{bmatrix},\n",
    "    \\qquad\n",
    "    \\mbox{sign}(t) = \n",
    "        \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 &\\mbox{if } t>0 \\\\\n",
    "            -1 &\\mbox{if } t<0 \\\\\n",
    "            0 &\\mbox{if } t=0.\n",
    "        \\end{array}\n",
    "        \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tailored subgradient algorithm for robust linear regression\n",
    "def sub_grad(X,y,w0,stepchoice=0,step0=1, n_iter=4000,verbose=False): \n",
    "    \"\"\"\n",
    "        Subgradient algorithm for robust linear regression with l1 loss.\n",
    "        \n",
    "        Inputs:\n",
    "            X: Data matrix (features)\n",
    "            y: Data vector (labels)\n",
    "            w0: Initial point\n",
    "            stepchoice: Stepsize choice\n",
    "                0: Constant proportional to 1/L (L Lipschitz constant for the gradient)\n",
    "                a>0: Decreasing, set to 1/((k+1)**a)\n",
    "            step0: Initial stepsize\n",
    "            n_iter: Maximum number of iterations\n",
    "            verbose: Plot information at the iteration level?\n",
    "            \n",
    "        Outputs:\n",
    "            w_output: Last iterate\n",
    "            objvals: History of function values at the iterates\n",
    "            objavg: History of function values at the averatges\n",
    "    \"\"\"\n",
    "    \n",
    "    ############\n",
    "    # Initialization\n",
    "    objvals = []\n",
    "    objavg = []\n",
    "    \n",
    "    \n",
    "    # Dimensions\n",
    "    (n,d) = X.shape\n",
    "    \n",
    "    # Initial point\n",
    "    w = w0.copy()\n",
    "    \n",
    "    # Average of iterates\n",
    "    wavg=w0.copy()\n",
    "\n",
    "    k=0\n",
    "    \n",
    "    obj = norm(X.dot(w)-y,1) \n",
    "    objvals.append(obj)\n",
    "    objavg.append(obj)\n",
    "    \n",
    "    if verbose:\n",
    "        # Optional display\n",
    "        print(\"Subgradient, robust regression\",n)\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"favg\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % obj).rjust(8)]))\n",
    "    \n",
    "    ################\n",
    "    # Main loop\n",
    "    while (k < n_iter):\n",
    "        \n",
    "        \n",
    "        # Compute subgradient\n",
    "        sg = (1/n)*X.T@np.sign(X.dot(w)-y)\n",
    "            \n",
    "        if stepchoice==0:\n",
    "            w[:] = w - step0 * sg\n",
    "        elif stepchoice>0:\n",
    "            sk = float(step0/((k+1)**stepchoice))\n",
    "            w[:] = w - sk * sg\n",
    "            \n",
    "        obj = norm(X.dot(w)-y,1)\n",
    "        \n",
    "        # Maintain a running average of the iterates\n",
    "        wavg = k/(k+1) *wavg + w/(k+1) \n",
    "        obj_wavg = norm(X.dot(wavg)-y,1)\n",
    "        \n",
    "\n",
    "        k += 1\n",
    "        \n",
    "        # Record objective values for both the iterate sequence and the average sequence\n",
    "        objvals.append(obj)\n",
    "        objavg.append(obj_wavg)\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % obj_wavg).rjust(8)]))       \n",
    "    \n",
    "    # End main loop\n",
    "    #################\n",
    "    \n",
    "    # Solution\n",
    "    w_output = w.copy()\n",
    "    \n",
    "    return w_output, np.array(objvals), np.array(objavg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands on! Subgradient</span>\n",
    "\n",
    "**Run the subgradient method below with different constant stepsize choices. Use at least one value <1 and one value $\\ge$ 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments on subgradient methods\n",
    "\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "# Choose stepsizes greater and smaller than 1\n",
    "cst_step = []\n",
    "\n",
    "\n",
    "nvals = len(cst_step)\n",
    "nits = 4000\n",
    "\n",
    "objs = np.zeros((nits+1,nvals))\n",
    "avgs = np.zeros((nits+1,nvals))\n",
    "\n",
    "for val in range(nvals):\n",
    "    _, objs[:,val], avgs[:,val] = sub_grad(X,y,w0,stepchoice=0,step0=cst_step[val],n_iter=nits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting objective values (iterates+average)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.set_cmap(\"RdPu\")\n",
    "for val in range(nvals):\n",
    "    plt.semilogy(objs[:,val], label=\"alpha=\"+str(cst_step[val]), lw=2)\n",
    "plt.title(\"Objective values (iterates)\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log)\", fontsize=14)\n",
    "plt.legend(loc=1)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.set_cmap(\"RdPu\")\n",
    "for val in range(nvals):\n",
    "    plt.semilogy(avgs[:,val], label=\"alpha=\"+str(cst_step[val]), lw=2)\n",
    "plt.title(\"Objective values (average)\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log)\", fontsize=14)\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1.1 - C. W. Royer, February 2025."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
