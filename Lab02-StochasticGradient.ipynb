{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">CIMPA School Research School \"Control, Optimization, and Model Reduction in Machine Learning\"</span>\n",
    "\n",
    "### <span style=\"color:rgb(139,69,19)\">Optimization for Machine Learning - C. W. Royer</span>\n",
    "\n",
    "\n",
    "# <span style=\"color:rgb(139,69,19)\">Lab 02 - Stochastic gradient</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(139,69,19)\">Preliminary remarks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab session is to implement several variants of the stochastic gradient method. As in the previous lab session, we will try those on two different regression problems that possess a finite-sum structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble: useful toolboxes, librairies, functions, etc.\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt # Square root\n",
    "\n",
    "# NumPy - Matrix and vector structures\n",
    "import numpy as np # NumPy library\n",
    "from numpy.random import multivariate_normal, randn # Probability distributions on vectors\n",
    "\n",
    "# SciPy - Efficient mathematical calculation\n",
    "from scipy.linalg import toeplitz # A special kind of matrices\n",
    "from scipy.linalg import svdvals # Singular values\n",
    "from scipy.linalg import norm # Euclidean norm\n",
    "from scipy.optimize import check_grad # Check accuracy between objective and gradient values\n",
    "from scipy.optimize import fmin_l_bfgs_b # Efficient optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">Part 1 - Data generation and finite-sum problems</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we restate our setup from the first lab session. Recall that our results were based upon a dataset $\\{(\\mathbf{a}_i,y_i)\\}_{i=1,\\dots,n}$, where $a_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$, available in the form of:\n",
    "\n",
    "- a feature matrix $A \\in \\mathbb{R}^{n \\times d}$;\n",
    "- and a vector of labels $y \\in \\mathbb{R}^n$. \n",
    "\n",
    "Given this dataset, we seek a model parameterized by a vector $w$ that explains the data according to a certain loss function. This results in the following formulation:\n",
    "$$\n",
    "    \\min_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{x}), \\qquad f_i(\\mathbf{x}) = \\ell(h(\\mathbf{a}_i,\\mathbf{x}),y_i) + \\frac{\\lambda}{2}\\|\\mathbf{x}\\|^2.\n",
    "$$\n",
    "where $\\lambda \\ge 0$ is an optional regularization parameter *(more on this in the lectures on proximal gradient and LASSO)*.\n",
    "\n",
    "The dataset will be produced according to the procedure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation.\n",
    "# This code is inspired by a generator proposed by A. Gramfort.\n",
    "\n",
    "def simu_linmodel(w, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation values obtained by a linear model with additive noise\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the feature matrix\n",
    "    \"\"\"    \n",
    "    d = w.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    X = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    y = X.dot(w) + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is thus produced from a linear model corrupted with (Gaussian) noise. Note that the feature vectors are correlated so as to increase interest of stochastic gradient approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\"> Regression models</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a generic regression problem encoding both linear and logistic regression tasks.\n",
    "\n",
    "- In linear regression, our model is linear and the loss is the least-squares (or $\\ell_2$ loss). We thus have\n",
    "$$\n",
    "    f(\\mathbf{w}) = \\frac{1}{2 n} \\|\\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2, \n",
    "    \\quad\n",
    "    f_i(\\mathbf{w}) = \\frac{1}{2} (\\mathbf{a}_i^T \\mathbf{w} - y_i)^2 + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2.\n",
    "$$\n",
    "The function $f$ is $\\mathcal{C}^{1,1}_L$ with $L=\\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|}{n}+\\lambda$ and\n",
    "$$\n",
    "    \\nabla f(\\mathbf{w})=\\frac{1}{n}\\mathbf{X}^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) + \\lambda \\mathbf{x}.\n",
    "$$\n",
    "We can also establish that every $f_i$ is $\\mathcal{C}^1$ with\n",
    "$$\n",
    "    \\nabla f_i(\\mathbf{w}) = (\\mathbf{x}_i^T \\mathbf{w}-y_i)\\mathbf{x}_i + \\lambda\\mathbf{w}.\n",
    "$$\n",
    "- In logistic regression, our goal is to classify correctly the data points. Our model is still linear but we now consider a sigmoid loss:\n",
    "$$\n",
    "    f(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n f_i(\\mathbf{w}), \\quad \n",
    "    f_i(\\mathbf{w}) = \\log(1+\\exp(-y_i \\mathbf{x}_i^T \\mathbf{w}))+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2.\n",
    "$$\n",
    "We have already seen that $f$ is $\\mathcal{C}^{1,1}_L$ with\n",
    "$$\n",
    "\\nabla f(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n  -\\frac{y_i}{1 + \\exp(y_i \\mathbf{x}_i^T \\mathbf{w})} \\mathbf{x}_i + \\lambda \\mathbf{w}\n",
    "$$\n",
    "and $L:=\\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|}{4n}+\\lambda$. Every $f_i$ is also $\\mathcal{C}^1$ \n",
    "$$\n",
    "\\nabla f_i(\\mathbf{w}) = - \\frac{y_i}{1 + \\exp(y_i \\mathbf{x}_i^T \\mathbf{w})} \\mathbf{x}_i + \\lambda \\mathbf{w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python class for regression problems\n",
    "class RegPb(object):\n",
    "    '''\n",
    "        A class for regression problems with linear models.\n",
    "        \n",
    "        Attributes:\n",
    "            X: Data matrix (features)\n",
    "            y: Data vector (labels)\n",
    "            n,d: Dimensions of X\n",
    "            loss: Loss function to be considered in the regression\n",
    "                'l2': Least-squares loss\n",
    "                'logit': Logistic loss\n",
    "            lbda: Regularization parameter\n",
    "    '''\n",
    "   \n",
    "    # Instantiate the class\n",
    "    def __init__(self, X, y,lbda=0,loss='l2'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "        self.loss = loss\n",
    "        self.lbda = lbda\n",
    "        \n",
    "    \n",
    "    # Objective value\n",
    "    def fun(self, w):\n",
    "        if self.loss=='l2':\n",
    "            return norm(self.X.dot(w) - self.y) ** 2 / (2. * self.n) + self.lbda * norm(w) ** 2 / 2.\n",
    "        elif self.loss=='logit':\n",
    "            yXw = self.y * self.X.dot(w)\n",
    "            return np.mean(np.log(1. + np.exp(-yXw))) + self.lbda * norm(w) ** 2 / 2.\n",
    "    \n",
    "    # Partial objective value\n",
    "    def f_i(self, i, w):\n",
    "        if self.loss=='l2':\n",
    "            return norm(self.X[i].dot(w) - self.y[i]) ** 2 / (2.) + self.lbda * norm(w) ** 2 / 2.\n",
    "        elif self.loss=='logit':\n",
    "            yWxi = self.y[i] * np.dot(self.X[i], w)\n",
    "            return np.log(1. + np.exp(- yXwi)) + self.lbda * norm(w) ** 2 / 2.\n",
    "    \n",
    "    # Full gradient computation\n",
    "    def grad(self, w):\n",
    "        if self.loss=='l2':\n",
    "            return self.X.T.dot(self.X.dot(w) - self.y) / self.n + self.lbda * w\n",
    "        elif self.loss=='logit':\n",
    "            yXw = self.y * self.X.dot(w)\n",
    "            aux = 1. / (1. + np.exp(yXw))\n",
    "            return - (self.X.T).dot(self.y * aux) / self.n + self.lbda * w\n",
    "    \n",
    "    # Partial gradient\n",
    "    def grad_i(self,i,w):\n",
    "        x_i = self.X[i]\n",
    "        if self.loss=='l2':\n",
    "            return (x_i.dot(w) - self.y[i]) * x_i + self.lbda*w\n",
    "        elif self.loss=='logit':\n",
    "            grad = - x_i * self.y[i] / (1. + np.exp(self.y[i]* x_i.dot(w)))\n",
    "            grad += self.lbda * w\n",
    "            return grad     \n",
    "\n",
    "    # Lipschitz constant for the gradient\n",
    "    def lipgrad(self):\n",
    "        if self.loss=='l2':\n",
    "            L = norm(self.X, ord=2) ** 2 / self.n + self.lbda\n",
    "        elif self.loss=='logit':\n",
    "            L = norm(self.X, ord=2) ** 2 / (4. * self.n) + self.lbda\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the problem instances - we use moderate sizes but those will serve our purpose\n",
    "\n",
    "d = 50\n",
    "n = 1000\n",
    "idx = np.arange(d)\n",
    "lbda = 1. / n ** (0.5)\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "w_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "Xlin, ylin = simu_linmodel(w_model_truth, n, std=1., corr=0.1)\n",
    "Xlog, ylog = simu_linmodel(w_model_truth, n, std=1., corr=0.7)\n",
    "ylog = np.sign(ylog) # Taking the logarithm for binary classification\n",
    "\n",
    "pblinreg = RegPb(Xlin, ylin,lbda,loss='l2')\n",
    "pblogreg = RegPb(Xlog, ylog,lbda,loss='logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we work with relatively simple loss functions: we can thus efficiently compute a solution using a second-order method. This provides us with a target objective value as well as a target vector of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use L-BFGS-B to determine a solution for both problems\n",
    "\n",
    "w_init = np.zeros(d)\n",
    "# Compute the optimal solution for linear regression\n",
    "w_min_lin, f_min_lin, _ = fmin_l_bfgs_b(pblinreg.fun, w_init, pblinreg.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "print(\"Linear regression:\")\n",
    "print(\"\\t Numerical minimal value:\",f_min_lin)\n",
    "print(\"\\t Numerical minimum gradient norm:\",norm(pblinreg.grad(w_min_lin)))\n",
    "\n",
    "# Compute the optimal solution for logistic regression\n",
    "w_min_log, f_min_log, _ = fmin_l_bfgs_b(pblogreg.fun, w_init, pblogreg.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "print(\"Logistic regression:\")\n",
    "print(\"\\t Numerical minimal value:\",f_min_log)\n",
    "print(\"\\t Numerical minimum gradient norm:\",norm(pblogreg.grad(w_min_log)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These solutions will enable us to study the behavior of the distance to optimality in terms of function values \n",
    "$f(\\mathbf{w}_k)-f^*$ and iterates $\\|\\mathbf{w}_k -\\mathbf{w}^*\\|$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">Part 2 - Gradient and stochastic gradient methods</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined our problems, we now build a generic stochastic gradient method for comparison with gradient descent. We will then try to tune the batch size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\"> 2.1 Generic stochastic gradient framework</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iteration of stochastic gradient (also called *Stochastic Gradient Descent*, or *SGD*) is given by:\n",
    "\n",
    "$$\n",
    "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha_k \\nabla f_{i_k}(\\mathbf{w}_k),\n",
    "$$\n",
    "\n",
    "where $i_k$ is drawn at random in $\\{1,\\dots,n\\}$. For the purpose of this lab session, $i_k$ will be drawn uniformly at random.\n",
    "\n",
    "A more general version of stochastic gradient, called batch stochastic gradient, is given by the iteration\n",
    "\n",
    "$$\n",
    "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\frac{\\alpha_k}{|S_k|} \\sum_{i \\in S_k} \\nabla f_i(\\mathbf{w}_k)\n",
    "$$\n",
    "\n",
    "where $S_k$ is a set of indices drawn uniformly in $\\{1,\\dots,n\\}$. For this lab, the samples will be drawn without replacement, so that $|S_k|=n$ results in a full gradient step, while $|S_k|=1$ corresponds to a basic stochastic gradient step. In this notebook, we will focus on using the same batch size across all iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithmic template below implements a general batch stochastic gradient method. We include the possibility for two stepsize choices:\n",
    " - *$\\alpha_k=\\tfrac{1}{L}$, and* \n",
    " - *$\\alpha_k=\\frac{\\alpha_0}{\\sqrt{k+1}}$, where $\\alpha_0$ is an input parameter of the method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic gradient implementation\n",
    "def stoch_grad(w0,problem,wtarget,stepchoice=0,step0=1, n_iter=1000,nb=1,with_replace=False,verbose=True): \n",
    "    \"\"\"\n",
    "        A code for gradient descent with various step choices.\n",
    "        \n",
    "        Inputs:\n",
    "            w0: Initial vector\n",
    "            problem: Problem structure\n",
    "                problem.fun() returns the objective function, which is assumed to be a finite sum of functions\n",
    "                problem.n returns the number of components in the finite sum\n",
    "                problem.grad_i() returns the gradient of a single component f_i\n",
    "                problem.lipgrad() returns the Lipschitz constant for the gradient\n",
    "                problem.cvxval() returns the strong convexity constant\n",
    "                problem.lambda returns the value of the regularization parameter\n",
    "            wtarget: Target minimum (unknown in practice!)\n",
    "            stepchoice: Strategy for computing the stepsize \n",
    "                0: Constant step size equal to 1/L\n",
    "                t>0: Step size decreasing in 1/(k+1)**t\n",
    "            step0: Initial steplength (only used when stepchoice is not 0)\n",
    "            n_iter: Number of iterations, used as stopping criterion\n",
    "            nb: Number of components drawn per iteration/Batch size \n",
    "                1: Classical stochastic gradient algorithm (default value)\n",
    "            with_replace: Boolean indicating whether components are drawn with or without replacement\n",
    "                True: Components drawn with replacement\n",
    "                False: Components drawn without replacement (Default)\n",
    "            verbose: Boolean indicating whether information should be plot at every iteration (Default: False)\n",
    "            \n",
    "        Outputs:\n",
    "            x_output: Final iterate of the method (or average if average=1)\n",
    "            objvals: History of function values (Numpy array of length n_iter at most)\n",
    "            normits: History of distances between iterates and optimum (Numpy array of length n_iter at most)\n",
    "    \"\"\"\n",
    "    ############\n",
    "    # Initial step: Compute and plot some initial quantities\n",
    "\n",
    "    # objective history\n",
    "    objvals = []\n",
    "    \n",
    "    # iterates distance to the minimum history\n",
    "    normits = []\n",
    "    \n",
    "    # Lipschitz constant\n",
    "    L = problem.lipgrad()\n",
    "    \n",
    "    # Number of samples\n",
    "    n = problem.n\n",
    "    \n",
    "    # Initial value of current iterate  \n",
    "    w = w0.copy()\n",
    "    nw = norm(w)\n",
    "\n",
    "    # Initialize iteration counter\n",
    "    k=0\n",
    "    \n",
    "    # Current objective\n",
    "    obj = problem.fun(w) \n",
    "    objvals.append(obj);\n",
    "    # Current distance to the optimum\n",
    "    nmin = norm(w-wtarget)\n",
    "    normits.append(nmin)\n",
    "    \n",
    "    if verbose:\n",
    "        # Plot initial quantities of interest\n",
    "        print(\"Stochastic Gradient, batch size=\",nb,\"/\",n)\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"normit\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
    "    \n",
    "    ################\n",
    "    # Main loop\n",
    "    while (k < n_iter and nw < 10**100):\n",
    "        \n",
    "        ############################################\n",
    "        # Computing stochastic gradient\n",
    "        \n",
    "        # Draw the batch indices\n",
    "        ik = np.random.choice(n,nb,replace=with_replace)# Batch gradient\n",
    "        # Stochastic gradient calculation\n",
    "        sg = np.zeros(d)\n",
    "        for j in range(nb):\n",
    "            gi = problem.grad_i(ik[j],w)\n",
    "            sg = sg + gi\n",
    "        sg = (1/nb)*sg\n",
    "        \n",
    "        #############################################\n",
    "            \n",
    "        if stepchoice==0:\n",
    "            w[:] = w - (step0/L) * sg\n",
    "        elif stepchoice>0:\n",
    "            sk = float(step0/((k+1)**stepchoice))\n",
    "            w[:] = w - sk * sg\n",
    "        \n",
    "        nw = norm(w) #Computing the norm to measure divergence \n",
    "\n",
    "        obj = problem.fun(w)\n",
    "        nmin = norm(w-wtarget)\n",
    "        \n",
    "       \n",
    "        \n",
    "        k += 1\n",
    "        # Plot quantities of interest at the end of every epoch only\n",
    "        if (k*nb) % n == 0:\n",
    "            objvals.append(obj)\n",
    "            normits.append(nmin)\n",
    "            if verbose:\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))       \n",
    "    \n",
    "    # End of main loop\n",
    "    #################\n",
    "    \n",
    "    # Plot quantities of interest for the last iterate (if needed)\n",
    "    if (k*nb) % n > 0:\n",
    "        objvals.append(obj)\n",
    "        normits.append(nmin)\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))              \n",
    "    \n",
    "    # Outputs\n",
    "    w_output = w.copy()\n",
    "    \n",
    "    return w_output, np.array(objvals), np.array(normits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\"> 2.2 Gradient descent vs Stochastic gradient on logistic regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands-on! Gradient descent VS Stochastic gradient</span>\n",
    "\n",
    "**Run the script below to compare stochastic gradient and gradient descent on the logistic regression problem with 30 epochs and the step size strategies $\\alpha_k = \\tfrac{1}{L}$ and $\\alpha_k = \\tfrac{0.2}{\\sqrt{k+1}}$. What is your interpretation of those curves?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare implementations of gradient descent/stochastic gradient\n",
    "# Pay attention to the budget allocated to each solver (the cost of one iteration of gradient descent vs \n",
    "# the cost of 1 iteration of stochastic gradient are different)\n",
    "\n",
    "nb_epochs = 60\n",
    "n = pblinreg.n\n",
    "nbset = 1\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "# Run a - Gradient descent with constant stepsize\n",
    "_, obj_a, nits_a = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0,step0=1, n_iter=nb_epochs,nb=n)\n",
    "\n",
    "# Run b - Stochastic gradient with constant stepsize\n",
    "# The version below may diverges, in which case the bound on norm(w) in the code will be triggered\n",
    "_, obj_b, nits_b = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0,step0=1, n_iter=int(nb_epochs*n/nbset),nb=1)\n",
    "\n",
    "# Run c - Gradient descent with decreasing stepsize\n",
    "_, obj_c, nits_c = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0.5,step0=0.2, n_iter=nb_epochs,nb=n)\n",
    "\n",
    "# Run d - Stochastic gradient with decreasing stepsize\n",
    "_, obj_d, nits_d = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of variants of GD/SG with the same stepsize rule\n",
    "# NB: The x-axis is in epochs (1 iteration of GD).\n",
    "\n",
    "# In terms of objective value (logarithmic scale)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_a-f_min_log, label=\"GD - 1/L\", lw=2)\n",
    "plt.semilogy(obj_b-f_min_log, label=\"SG - 1/L\", lw=2)\n",
    "plt.semilogy(obj_c-f_min_log, label=\"GD - 1/sqrt(k+1)\", lw=2)\n",
    "plt.semilogy(obj_d-f_min_log, label=\"SG - 1/sqrt(k+1)\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\"> 2.3 Experimenting with the step size/learning rate</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run several instances of vanilla stochastic gradient with constant step size proportional to $\\frac{1}{L}$ (using the ``step0`` parameter from the code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands-on! Tuning the learning rate</span>\n",
    "\n",
    "**1) Run vanilla stochastic gradient with various constant values for the stepsize chosen to be proportional to $\\frac{1}{L}$. What do you observe?**\n",
    "\n",
    "**2) Run vanilla stochastic gradient with decreasing stepsizes of the form $\\frac{\\alpha_0}{(k+1)^t}$. What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run several instances of stochastic gradient with constant batch size\n",
    "\n",
    "nb_epochs = 60\n",
    "n = pblinreg.n\n",
    "nbset = 1\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "########################\n",
    "# Input your choices of scaling factor for the stepsizes here\n",
    "valsstep0 = []\n",
    "##########################################\n",
    "nvals = len(valsstep0)\n",
    "\n",
    "objs = np.zeros((nb_epochs+1,nvals))\n",
    "\n",
    "for val in range(nvals):\n",
    "    _, objs[:,val], _ = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0,step0=valsstep0[val], n_iter=int(nb_epochs*n/nbset),nb=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of variants of SG with different (constant) stepsizes\n",
    "# NB: The x-axis is in epochs (1 iteration of GD).\n",
    "\n",
    "# In terms of objective value (logarithmic scale)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.set_cmap(\"RdPu\")\n",
    "for val in range(nvals):\n",
    "    plt.semilogy(objs[:,val]-f_min_log, label=\"SG -\"+str(valsstep0[val])+\"/L\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run several instances of stochastic gradient with decreasing step sizes\n",
    "\n",
    "nb_epochs = 60\n",
    "n = pblinreg.n\n",
    "nbset = 1\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "###########################\n",
    "# Input your choices for t to define a stepsize sequence {step0/(k+1)^t}\n",
    "decstep = []\n",
    "###########################\n",
    "\n",
    "nvals = len(decstep)\n",
    "\n",
    "objs = np.zeros((nb_epochs+1,nvals))\n",
    "\n",
    "for val in range(nvals):\n",
    "    _, objs[:,val], _ = stoch_grad(w0,pblogreg,w_min_log,stepchoice=decstep[val],step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of variants of SG with different (constant) stepsizes\n",
    "# NB: The x-axis is in epochs (1 iteration of GD).\n",
    "\n",
    "# In terms of objective value (logarithmic scale)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.set_cmap(\"RdPu\")\n",
    "for val in range(nvals):\n",
    "    plt.semilogy(objs[:,val]-f_min_log, label=\"SG -(k+1)^\"+str(decstep[val]), lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\"> 2.4 Experimenting with the batch size</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to compare the performance of stochastic gradient with several values for the batch size (using decreasing stepsizes) using the same epoch budget\n",
    "\n",
    "***NB: One must pay attention to the definition of the number of iterations for each variant, as it depends on the batch size.***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands-on! Batch size tuning</span>\n",
    "\n",
    "**1) Run the method with $n_b \\in \\left\\{1,\\tfrac{n}{100},\\tfrac{n}{10},\\tfrac{n}{2},n \\right\\}$ (NB: $n=1000$ in the default settings) without replacement, to include stochastic gradient ($n_b=1$) and gradient descent ($n_b=n$ without replacement).**\n",
    "\n",
    "**2) Can you find a better value for the batch size than those above?**\n",
    "\n",
    "**3) Do the conclusions change when the indices are drawn with replacement?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test several values for the batch size (with replacement) using the same epoch budget.\n",
    "\n",
    "nb_epochs = 100\n",
    "n = pblogreg.n\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "replace_batch=False\n",
    "\n",
    "# Stochastic gradient (batch size 1)\n",
    "_, obj_a, nits_a = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1)\n",
    "\n",
    "# Batch stochastic gradient (batch size n/100)\n",
    "nbset=int(n/100)\n",
    "_, obj_b, nits_b = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
    "# Batch stochastic gradient (batch size n/10)\n",
    "nbset=int(n/10)\n",
    "_, obj_c, nits_c = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
    "# Batch stochastic gradient (batch size n/2)\n",
    "nbset=int(n/2)\n",
    "_, obj_d, nits_d = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
    "\n",
    "# Gradient descent (batch size n, taken without replacement)\n",
    "_, obj_e, nits_e = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs),nb=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of variants of batch SGD with the same stepsize rule\n",
    "\n",
    "# In terms of objective value (logarithmic scale)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_a-f_min_lin, label=\"SG (batch=1)\", lw=2)\n",
    "plt.semilogy(obj_b-f_min_lin, label=\"Batch SG - n/100\", lw=2)\n",
    "plt.semilogy(obj_c-f_min_lin, label=\"Batch SG - n/10\", lw=2)\n",
    "plt.semilogy(obj_d-f_min_lin, label=\"Batch SG - n/2\", lw=2)\n",
    "plt.semilogy(obj_e-f_min_lin, label=\"GD\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test several values for the batch size (with replacement) using the same epoch budget.\n",
    "\n",
    "nb_epochs = 100\n",
    "n = pblogreg.n\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "replace_batch=True\n",
    "\n",
    "# Stochastic gradient (batch size 1)\n",
    "_, obj_ar, nits_ar = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1)\n",
    "# Batch stochastic gradient (batch size n/100)\n",
    "nbset=int(n/100)\n",
    "_, obj_br, nits_br = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
    "# Batch stochastic gradient (batch size n/10)\n",
    "nbset=int(n/10)\n",
    "_, obj_cr, nits_cr = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
    "# Batch stochastic gradient (batch size n/2)\n",
    "nbset=int(n/2)\n",
    "_, obj_dr, nits_dr = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
    "\n",
    "# Gradient descent (batch size n, taken with replacement)\n",
    "_, obj_er, nits_er = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs),nb=n,with_replace=replace_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of variants of batch SGD with the same stepsize rule\n",
    "\n",
    "# In terms of objective value (logarithmic scale)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_ar-f_min_lin, label=\"SG (batch=1 wr)\", lw=2)\n",
    "plt.semilogy(obj_a-f_min_lin, label=\"SG (batch=1)\", lw=2)\n",
    "plt.semilogy(obj_br-f_min_lin, label=\"Batch SG - n/100 (wr)\", lw=2)\n",
    "plt.semilogy(obj_b-f_min_lin, label=\"Batch SG - n/100\", lw=2)\n",
    "plt.semilogy(obj_cr-f_min_lin, label=\"Batch SG - n/10 (wr)\", lw=2)\n",
    "plt.semilogy(obj_c-f_min_lin, label=\"Batch SG - n/10\", lw=2)\n",
    "plt.semilogy(obj_dr-f_min_lin, label=\"Batch SG - n/2 (wr)\", lw=2)\n",
    "plt.semilogy(obj_d-f_min_lin, label=\"Batch SG - n/2\", lw=2)\n",
    "plt.semilogy(obj_er-f_min_lin, label=\"Batch SG - n (wr)\", lw=2)\n",
    "plt.semilogy(obj_e-f_min_lin, label=\"GD\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Bonus experiment</span> \n",
    "\n",
    "As a bonus experiment, we compare batch stochastic gradient techniques over 10 random runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test several values for the batch size using the same epoch budget.\n",
    "\n",
    "nb_epochs = 100\n",
    "n = pblinreg.n\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "nruns = 10\n",
    "\n",
    "for i in range(nruns):\n",
    "    ############################\n",
    "    # Run standard stochastic gradient (batch size 1)\n",
    "    _, obj_a, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,with_replace=True,verbose=False)\n",
    "    # Batch stochastic gradient (batch size n/10)\n",
    "    nbset=int(n/10)\n",
    "    _, obj_b, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=True,verbose=False)\n",
    "    # Batch stochastic gradient (batch size n/2)\n",
    "    nbset=int(n/2)\n",
    "    _, obj_c, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=True,verbose=False)\n",
    "    # Batch stochastic gradient (batch size n, with replacement)\n",
    "    nbset=n\n",
    "    _, obj_d, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=True,verbose=False)\n",
    "    ############################\n",
    "    \n",
    "    # Plots runs on the same figure\n",
    "    if i<nruns-1:\n",
    "        plt.semilogy(obj_a-f_min_lin,color='orange',lw=2)\n",
    "        plt.semilogy(obj_b-f_min_lin,color='green', lw=2)\n",
    "        plt.semilogy(obj_c-f_min_lin,color='red', lw=2)\n",
    "        plt.semilogy(obj_d-f_min_lin,color='blue', lw=2)\n",
    "plt.semilogy(obj_a-f_min_lin,label=\"SG\",color='orange',lw=2)\n",
    "plt.semilogy(obj_b-f_min_lin,label=\"batch n/10\",color='green',lw=2)\n",
    "plt.semilogy(obj_c-f_min_lin,label=\"batch n/2\",color='red', lw=2)\n",
    "plt.semilogy(obj_d-f_min_lin,label=\"batch n\",color='blue', lw=2)    \n",
    "\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(139,69,19)\">Part 3 - Variants on the stochastic gradient framework</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to present popular variants of the classical stochastic gradient scheme. The augmented code below aims at adding the following feature The following implementation will be used throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:rgb(139,69,19)\">3.1 Practical SG variants based on diagonal scaling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(139,69,19)\">About *RMSProp* and *Adagrad*</span>\n",
    "\n",
    "*RMSProp* and *Adagrad* are both based on diagonal scaling. This corresponds to rescaling the stochastic gradient step componentwise as follows\n",
    "\n",
    " $$\n",
    "     [\\mathbf{w}_{k+1}]_i  = [\\mathbf{w}_k]_i -\\frac{\\alpha_k}{\\sqrt{[\\mathbf{r}_k]_i + \\epsilon}}[\\nabla f_{i_k}(\\mathbf{w}_k)]_i,\n",
    " $$\n",
    " \n",
    " where $\\epsilon>0$ is added to avoid numerical issues, and $\\mathbf{r}_k \\in \\mathbb{R}^d$ is defined recursively by $\\mathbf{r}_{-1} = 0_{\\mathbb{R}^d}$ and\n",
    " \n",
    " $$ \n",
    "     \\forall k \\ge 0,\\ \\forall i=1,\\dots,d, \\qquad \n",
    "     [\\mathbf{r}_k]_i = \n",
    "     \\left\\{\n",
    "         \\begin{array}{ll}\n",
    "             \\beta [\\mathbf{r}_{k-1}]_i + (1-\\beta) [\\nabla f_{i_k}(\\mathbf{w}_k)]_i^2 &\\mathrm{for\\ RMSProp,} \\\\\n",
    "             [\\mathbf{r}_{k-1}]_i + [\\nabla f_{i_k}(\\mathbf{w}_k)]_i^2 &\\mathrm{for\\ Adagrad.}\n",
    "         \\end{array}\n",
    "     \\right.\n",
    " $$\n",
    "(Suggested values: $\\epsilon=\\tfrac{1}{2 \\sqrt{n}}$, $\\beta=0.8$.)\n",
    "\n",
    "The use of $\\epsilon>0$ prevents the scaling of each component of the stochastic gradient from going to zero. *This technique is typically adopted in modern implementations of these methods.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced stochastic gradient implementation based on diagonal scaling\n",
    "def stoch_grad_scaling(w0,problem,wtarget,stepchoice=0,step0=1, n_iter=1000,nb=1,beta=0,with_replace=False,verbose=False): \n",
    "    \"\"\"\n",
    "        A code for gradient descent with various step choices.\n",
    "        \n",
    "        Inputs:\n",
    "            w0: Initial vector\n",
    "            problem: Problem structure\n",
    "                problem.fun() returns the objective function, which is assumed to be a finite sum of functions\n",
    "                problem.n returns the number of components in the finite sum\n",
    "                problem.grad_i() returns the gradient of a single component f_i\n",
    "                problem.lipgrad() returns the Lipschitz constant for the gradient\n",
    "                problem.cvxval() returns the strong convexity constant\n",
    "                problem.lambda returns the value of the regularization parameter\n",
    "            wtarget: Target minimum (unknown in practice!)\n",
    "            stepchoice: Strategy for computing the stepsize \n",
    "                0: Constant step size equal to 1/L\n",
    "                t>0: Step size decreasing in 1/(k+1)^t\n",
    "            step0: Initial steplength (only used when stepchoice is not 0)\n",
    "            n_iter: Number of iterations, used as stopping criterion\n",
    "            nb: Number of components drawn per iteration/Batch size \n",
    "                1: Classical stochastic gradient algorithm (default value)\n",
    "                problem.n: Classical gradient descent (default value)\n",
    "            beta: Use a diagonal scaling\n",
    "                0: No scaling (default)\n",
    "                (0,1): Average of magnitudes (RMSProp)\n",
    "                1: Normalization with magnitudes (Adagrad)\n",
    "            with_replace: Boolean indicating whether components are drawn with or without replacement\n",
    "                True: Components drawn with replacement\n",
    "                False: Components drawn without replacement (Default)\n",
    "            verbose: Boolean indicating whether information should be plot at every iteration (Default: False)\n",
    "            \n",
    "        Outputs:\n",
    "            w_output: Final iterate of the method (or average if average=1)\n",
    "            objvals: History of function values (Numpy array of length n_iter at most)\n",
    "            normits: History of distances between iterates and optimum (Numpy array of length n_iter at most)\n",
    "    \"\"\"\n",
    "    ############\n",
    "    # Initial step: Compute and plot some initial quantities\n",
    "\n",
    "    # objective history\n",
    "    objvals = []\n",
    "    \n",
    "    # iterates distance to the minimum history\n",
    "    normits = []\n",
    "    \n",
    "    # Lipschitz constant\n",
    "    L = problem.lipgrad()\n",
    "    \n",
    "    # Number of samples\n",
    "    n = problem.n\n",
    "    \n",
    "    # Initial value of current iterate  \n",
    "    w = w0.copy()\n",
    "    nw = norm(w)\n",
    "\n",
    "    \n",
    "    #Scaling values\n",
    "    if beta>0:\n",
    "        eps=1/(2 *(n ** (0.5))) # To avoid numerical issues\n",
    "        v = np.zeros(d)\n",
    "\n",
    "    # Initialize iteration counter\n",
    "    k=0\n",
    "    \n",
    "    # Current objective\n",
    "    obj = problem.fun(w) \n",
    "    objvals.append(obj);\n",
    "    # Current distance to the optimum\n",
    "    nmin = norm(w-wtarget)\n",
    "    normits.append(nmin)\n",
    "    \n",
    "    # Plot initial quantities of interest\n",
    "    if verbose:\n",
    "        print(\"Stochastic Gradient, batch size=\",nb,\"/\",n)\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"normit\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
    "    \n",
    "    ################\n",
    "    # Main loop\n",
    "    while (k < n_iter and nw < 10**100):\n",
    "        \n",
    "        #########################################\n",
    "        # Draw the batch indices\n",
    "        ik = np.random.choice(n,nb,replace=with_replace)# Batch gradient\n",
    "        # Stochastic gradient calculation\n",
    "        sg = np.zeros(d)\n",
    "        for j in range(nb):\n",
    "            gi = problem.grad_i(ik[j],w)\n",
    "            sg = sg + gi\n",
    "        sg = (1/nb)*sg\n",
    "        ###########################################\n",
    "        \n",
    "        ###########################################\n",
    "        # Scaling\n",
    "        if beta>0:\n",
    "            if beta==1:\n",
    "                # Adagrad update\n",
    "                v = v + sg*sg \n",
    "            else:\n",
    "                # RMSProp update\n",
    "                v = beta*v + (1-beta)*sg*sg\n",
    "            sg = sg/(np.sqrt(v+eps))\n",
    "        ##########################################\n",
    "            \n",
    "        if stepchoice==0:\n",
    "            w[:] = w - (step0/L) * sg\n",
    "        elif stepchoice>0:\n",
    "            sk = float(step0/((k+1)**stepchoice))\n",
    "            w[:] = w - sk * sg\n",
    "        \n",
    "        nx = norm(w) #Computing the norm to measure divergence \n",
    "        \n",
    "        obj = problem.fun(w)\n",
    "        nmin = norm(w-wtarget)\n",
    "       \n",
    "        \n",
    "        k += 1\n",
    "        # Plot quantities of interest at the end of every epoch only\n",
    "        if (k*nb) % n == 0:\n",
    "            objvals.append(obj)\n",
    "            normits.append(nmin)\n",
    "            if verbose:\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))     \n",
    "    \n",
    "    # End of main loop\n",
    "    #################\n",
    "    \n",
    "    # Plot quantities of interest for the last iterate (if needed)\n",
    "    if (k*nb) % n > 0:\n",
    "        objvals.append(obj)\n",
    "        normits.append(nmin)\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))              \n",
    "    \n",
    "    # Outputs\n",
    "    w_output = w.copy()\n",
    "    \n",
    "    return w_output, np.array(objvals), np.array(normits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands-on! Diagonal scaling</span> \n",
    "\n",
    "**Run the block below to compare RMSProp and Adagrad with SG using a decreasing stepsize $\\frac{\\alpha_0}{\\sqrt{k+1}}$. What do you observe?**\n",
    "\n",
    "**Can you improve the performance of RMSProp by playing with $\\beta$?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of stochastic gradient with and without diagonal scaling\n",
    "\n",
    "nb_epochs = 60\n",
    "n = pblinreg.n\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "# Stochastic gradient (batch size 1) without diagonal scaling\n",
    "_, obj_a, nits_a = stoch_grad_scaling(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,beta=0)\n",
    "\n",
    "# Stochastic gradient (batch size 1) with Adagrad diagonal scaling (decreasing stepsize)\n",
    "_, obj_bd, nits_bd = stoch_grad_scaling(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,beta=1)\n",
    "\n",
    "# Stochastic gradient (batch size 1) with RMSProp diagonal scaling (decreasing stepsize)\n",
    "_, obj_cd, nits_cd = stoch_grad_scaling(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,beta=0.8)\n",
    "\n",
    "# Stochastic gradient (batch size 1) with Adagrad diagonal scaling (constant stepsize)\n",
    "_, obj_bc, nits_bc = stoch_grad_scaling(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.2, n_iter=nb_epochs*n,nb=1,beta=1)\n",
    "\n",
    "# Stochastic gradient (batch size 1) with RMSProp diagonal scaling (constant stepsize)\n",
    "_, obj_cc, nits_cc = stoch_grad_scaling(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.2, n_iter=nb_epochs*n,nb=1,beta=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results - Comparison of stochastic gradient with and without diagonal scaling\n",
    "# In terms of objective value (logarithmic scale)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_a-f_min_lin, label=\"SG\", lw=2)\n",
    "plt.semilogy(obj_bd-f_min_lin, label=\"Adagrad (dec)\", lw=2)\n",
    "plt.semilogy(obj_cd-f_min_lin, label=\"RMSProp (dec)\", lw=2)\n",
    "plt.semilogy(obj_bc-f_min_lin, label=\"Adagrad (cst)\", lw=2)\n",
    "plt.semilogy(obj_cc-f_min_lin, label=\"RMSProp (cst)\", lw=2)\n",
    "plt.title(\"SG vs Adagrad/RMSProp\", fontsize=16)\n",
    "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
    "plt.legend()\n",
    "# In terms of distance to the minimum (logarithmic scale)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(nits_a, label=\"SG\", lw=2)\n",
    "plt.semilogy(nits_bd, label=\"Adagrad (dec)\", lw=2)\n",
    "plt.semilogy(nits_cd, label=\"RMSProp (dec)\", lw=2)\n",
    "plt.semilogy(nits_bc, label=\"Adagrad (cst)\", lw=2)\n",
    "plt.semilogy(nits_cc, label=\"RMSProp (cst)\", lw=2)\n",
    "plt.title(\"SG vs Adagrad/RMSProp\", fontsize=16)\n",
    "plt.xlabel(\"#epochs\", fontsize=14)\n",
    "plt.ylabel(\"Distance to minimum (log scale)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(139,69,19)\">3.2 Momentum-based techiques</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(139,69,19)\">SGD with momentum and Adam</span>\n",
    "\n",
    "The idea behind momentum is to leverage information from the past iterations, by combining previous steps with the step suggested by stochastic gradient.\n",
    "\n",
    "***Stochastic gradient with momentum*** has the following form\n",
    "\n",
    "$$\n",
    "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha_k \\mathbf{m}_k, \n",
    "    \\quad \\mathrm{where} \\quad\n",
    "    \\mathbf{m}_k = \\beta_1 \\mathbf{m}_{k-1} + (1-\\beta_1)\\nabla f_{i_k}(\\mathbf{w}_k),\n",
    "$$\n",
    "\n",
    "with $\\beta_1 \\in [0,1)$ (with $\\beta_1=0$, we recover the standard stochastic gradient technique).\n",
    "\n",
    "***Adam*** combines momentum and diagonal scaling ideas, and can be written as \n",
    "$$\n",
    "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha_k \\mathbf{m}_k \\oslash \\sqrt{\\mathbf{v}_k},\n",
    "$$\n",
    "with\n",
    "$$\n",
    "    \\mathbf{m}_k = \\frac{1-\\beta_1^k}{1-\\beta_1^{k+1}}\\beta_1\\mathbf{m}_{k-1} + \\frac{1-\\beta_1}{1-\\beta_1^{k+1}}\\nabla f_{i_k}(\\mathbf{w}_k)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\mathbf{v}_k = \\frac{1-\\beta_2^k}{1-\\beta_2^{k+1}}\\beta_2\\mathbf{v}_{k-1} + \\frac{1-\\beta_2}{1-\\beta_2^{k+1}}\\nabla f_{i_k}(\\mathbf{w}_k)\\otimes\\nabla f_{i_k}(\\mathbf{w}_k).\n",
    "$$\n",
    "\n",
    "In practice, the vector $\\mathbf{v}_k$ is replaced by $\\mathbf{v}_k+\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements these momentum-based techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic gradient with momentum\n",
    "def stoch_grad_momentum(w0,problem,wtarget,stepchoice=0,step0=1, n_iter=1000,nb=1,beta1=0.9,beta2=0.999,with_replace=False,verbose=False): \n",
    "    \"\"\"\n",
    "        A code for stochastic gradient with momentum and Adam\n",
    "        The code depends on two parameter beta1 and beta2.\n",
    "            \n",
    "            1) beta1=beta2=0 gives vanilla stochastic gradient.\n",
    "            2) beta1>0 and beta2=0 corresponds to stochastic gradient with momentum\n",
    "            3) beta1>0 and beta2>0 corresponds to Adam\n",
    "            \n",
    "            Nb: Choosing beta1=0 and beta2>0 would be more in the spirit of RMSProp above.\n",
    "        \n",
    "        Inputs:\n",
    "            w0: Initial point\n",
    "            problem: Instance to be minimized\n",
    "                problem.fun(x) Objective function            \n",
    "                problem.grad_i() Gradient of function f_i for finite sum\n",
    "                problem.lipgrad() Lipschitz constant for the gradient\n",
    "            wopt: Target minimum\n",
    "            stepchoice: Stepsize choice\n",
    "                0: Constant proportional to 1/L (L Lipschitz constant for the gradient)\n",
    "                a>0: Decreasing, set to 1/((k+1)**a)\n",
    "            step0: Initial stepsize\n",
    "            n_iter: Maximum number of iterations\n",
    "            nb: Batch size\n",
    "            beta1: Momentum parameter\n",
    "                0: Classical stochastic gradient direction (no momentum)\n",
    "                (0,1): Momentum-based method (défaut: 0.9)\n",
    "            beta2: Scaling parameter\n",
    "                0: No scaling (Same stepsize for each coordinate)\n",
    "                (0,1): Scaling every coordinate (default: 0.999) \n",
    "            with_replace: Drawn indices with replacement?\n",
    "            verbose: Plot iteration-dependent information\n",
    "            \n",
    "        Outputs:\n",
    "            x_output: Final iterate of the method (or average if average=1)\n",
    "            objvals: History of function values (Numpy array of length n_iter at most)\n",
    "            normits: History of distances between iterates and optimum (Numpy array of length n_iter at most)\n",
    "    \"\"\"\n",
    "\n",
    "    ############\n",
    "    # Initialization\n",
    "\n",
    "    # History of objective values and distance to the target minimum\n",
    "    objvals = []\n",
    "    normits = []\n",
    "    \n",
    "    # Lipschitz constant\n",
    "    L = problem.lipgrad()\n",
    "    \n",
    "    # Number of data points\n",
    "    n = problem.n\n",
    "    \n",
    "    # Initial point\n",
    "    w = w0.copy()\n",
    "    nw = norm(w)\n",
    "    \n",
    "    # Vector characterizing the direction\n",
    "    mv = np.zeros(d)\n",
    "    \n",
    "    # Scaling vector (if needed)\n",
    "    if beta2>0:\n",
    "        eps=0 #10**(-8) # Avoids numerical issues\n",
    "        v = np.zeros(d)\n",
    "\n",
    "    # Iteration count\n",
    "    k=0\n",
    "    \n",
    "    # Initial values\n",
    "    obj = problem.fun(w) \n",
    "    objvals.append(obj);\n",
    "    # \n",
    "    nmin = norm(w-wtarget)\n",
    "    normits.append(nmin)\n",
    "    \n",
    "    # Plotting information (optional)\n",
    "    if verbose:\n",
    "        if beta1>0:\n",
    "            if beta2>0:\n",
    "                print(\"Adam, batch size=\",nb,\"/\",n)\n",
    "            else:\n",
    "                print(\"SGD with momentum, batch size=\",nb,\"/\",n)\n",
    "        else:\n",
    "            print(\"Stochastic gradient, batch size=\",nb,\"/\",n)\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"normit\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
    "    \n",
    "    ################\n",
    "    # Main loop\n",
    "    while (k < n_iter and nw < 10**100):\n",
    "        \n",
    "        #########################################\n",
    "        # Draw indices\n",
    "        ik = np.random.choice(n,nb,replace=with_replace)\n",
    "        # Compute stochastic gradient estimate\n",
    "        sg = np.zeros(d)\n",
    "        for j in range(nb):\n",
    "            gi = problem.grad_i(ik[j],w)\n",
    "            sg = sg + gi\n",
    "        sg = (1/nb)*sg\n",
    "        ###########################################\n",
    "        \n",
    "        ###########################################\n",
    "        # Update the direction\n",
    "        if beta1>0:\n",
    "            if beta2>0:\n",
    "                mv = ((1-beta1**k)/(1-beta1**(k+1)))*beta1*mv + ((1-beta1)/(1-beta1**(k+1)))*sg\n",
    "            else:\n",
    "                mv = beta1*mv + (1-beta1)*sg\n",
    "        else:\n",
    "            mv = sg\n",
    "            \n",
    "        ###########################################\n",
    "        # Update scaling vector\n",
    "        if beta2>0:\n",
    "            v = ((1-beta2**k)/(1-beta2**(k+1)))*beta2*v + ((1-beta2)/(1-beta2**(k+1)))*sg*sg\n",
    "            if k>0:\n",
    "                mv = mv/(np.sqrt(v+eps))\n",
    "        ##########################################\n",
    "            \n",
    "        if stepchoice==0:\n",
    "            w[:] = w - (step0/L) * mv\n",
    "        elif stepchoice>0:\n",
    "            sk = float(step0/((k+1)**stepchoice))\n",
    "            w[:] = w - sk * mv\n",
    "        \n",
    "        nw = norm(w) # Compute norm to avoid divergence\n",
    "        \n",
    "        obj = problem.fun(w)\n",
    "        nmin = norm(w-wtarget)\n",
    "        \n",
    "        k += 1\n",
    "        # Affichage\n",
    "        if (k*nb) % n == 0:\n",
    "            objvals.append(obj)\n",
    "            normits.append(nmin)\n",
    "            if verbose:\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))     \n",
    "\n",
    "    \n",
    "    # End main loop\n",
    "    #################\n",
    "    \n",
    "    # Plotting information (optional)\n",
    "    if (k*nb) % n > 0:\n",
    "        objvals.append(obj)\n",
    "        normits.append(nmin)\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))              \n",
    "    \n",
    "    # Last iterate\n",
    "    w_output = w.copy()\n",
    "    \n",
    "    return w_output, np.array(objvals), np.array(normits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(139,69,19)\">Hands-on! Momentum</span> \n",
    "\n",
    "**1) Compare SG with momentum and Adam to SG using the defaults settings for Adam and $\\beta_1=0.9$ for SG with momentum.**\n",
    "\n",
    "**2) Play with the learning rate to try to improve the numerical performance of Adam.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical comparison\n",
    "nb_epochs = 100\n",
    "n = pblinreg.n\n",
    "w0 = np.zeros(d)\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(4)\n",
    "\n",
    "# Vanilla SG (decreasing stepsize)\n",
    "_, obj_sg_d, nits_sg_d = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,beta1=0,beta2=0)\n",
    "# SG with momentum (decreasing stepsize)\n",
    "_, obj_sgm_d, nits_sgm_d = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,beta1=0.9,beta2=0)\n",
    "# Adam (decreasing stepsize)\n",
    "_, obj_adam_d, nits_adam_d = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.01, n_iter=nb_epochs*n,nb=1)\n",
    "# SG (constant stepsize)\n",
    "_, obj_sg_c, nits_sg_c = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.001, n_iter=nb_epochs*n,nb=1,beta1=0,beta2=0)\n",
    "# SG with momentum (constant stepsize)\n",
    "_, obj_sgm_c, nits_sgm_c = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.001, n_iter=nb_epochs*n,nb=1,beta1=0.9,beta2=0)\n",
    "# Adam (constant stepsize)\n",
    "_, obj_adam_c, nits_adam_c = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.01, n_iter=nb_epochs*n,nb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "# In terms of function values\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sg_d-f_min_lin, label=\"SG (dec)\", lw=2)\n",
    "plt.semilogy(obj_sgm_d-f_min_lin, label=\"SG+momentum (dec)\", lw=2)\n",
    "plt.semilogy(obj_adam_d-f_min_lin, label=\"Adam (dec)\", lw=2)\n",
    "plt.semilogy(obj_sg_c-f_min_lin, label=\"SG (cst)\", lw=2)\n",
    "plt.semilogy(obj_sgm_c-f_min_lin, label=\"SG+momentum (cst)\", lw=2)\n",
    "plt.semilogy(obj_adam_c-f_min_lin, label=\"Adam (cst)\", lw=2)\n",
    "plt.title(\"SG vs Momentum\", fontsize=16)\n",
    "plt.xlabel(\"#epochs\", fontsize=14)\n",
    "plt.ylabel(\"Objective-target (log)\", fontsize=14)\n",
    "plt.legend()\n",
    "# In terms of distance to the target value\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(nits_sg_d, label=\"SG (dec)\", lw=2)\n",
    "plt.semilogy(nits_sgm_d, label=\"SG+momentum (dec)\", lw=2)\n",
    "plt.semilogy(nits_adam_d, label=\"Adam (dec)\", lw=2)\n",
    "plt.semilogy(nits_sg_c, label=\"SG (cst)\", lw=2)\n",
    "plt.semilogy(nits_sgm_c, label=\"SG+momentum (cst)\", lw=2)\n",
    "plt.semilogy(nits_adam_c, label=\"Adam (cst)\", lw=2)\n",
    "plt.title(\"SG vs Momentum\", fontsize=16)\n",
    "plt.xlabel(\"#epochs\", fontsize=14)\n",
    "plt.ylabel(\"Distance to minimum (log)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4.4 - C. W. Royer, February 2025."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
